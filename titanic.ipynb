{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 453, 'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.7}\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    1.6s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 446, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 15, 'bootstrap': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'iterations': 500, 'depth': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.76022 | train_auc: 0.62062 | valid_auc: 0.6913  |  0:00:01s\n",
      "epoch 1  | loss: 0.66933 | train_auc: 0.67657 | valid_auc: 0.76522 |  0:00:01s\n",
      "epoch 2  | loss: 0.63906 | train_auc: 0.70686 | valid_auc: 0.76943 |  0:00:02s\n",
      "epoch 3  | loss: 0.62493 | train_auc: 0.74422 | valid_auc: 0.7747  |  0:00:02s\n",
      "epoch 4  | loss: 0.59997 | train_auc: 0.76986 | valid_auc: 0.78485 |  0:00:03s\n",
      "epoch 5  | loss: 0.55949 | train_auc: 0.7872  | valid_auc: 0.79881 |  0:00:03s\n",
      "epoch 6  | loss: 0.56901 | train_auc: 0.80452 | valid_auc: 0.81555 |  0:00:04s\n",
      "epoch 7  | loss: 0.53828 | train_auc: 0.81798 | valid_auc: 0.82661 |  0:00:04s\n",
      "epoch 8  | loss: 0.53308 | train_auc: 0.82038 | valid_auc: 0.83557 |  0:00:05s\n",
      "epoch 9  | loss: 0.5148  | train_auc: 0.83022 | valid_auc: 0.83597 |  0:00:06s\n",
      "epoch 10 | loss: 0.48078 | train_auc: 0.83839 | valid_auc: 0.83702 |  0:00:07s\n",
      "epoch 11 | loss: 0.50839 | train_auc: 0.84374 | valid_auc: 0.82688 |  0:00:07s\n",
      "epoch 12 | loss: 0.4952  | train_auc: 0.85004 | valid_auc: 0.83136 |  0:00:08s\n",
      "epoch 13 | loss: 0.4988  | train_auc: 0.85881 | valid_auc: 0.83623 |  0:00:08s\n",
      "epoch 14 | loss: 0.45489 | train_auc: 0.86603 | valid_auc: 0.84321 |  0:00:09s\n",
      "epoch 15 | loss: 0.45453 | train_auc: 0.87184 | valid_auc: 0.84677 |  0:00:09s\n",
      "epoch 16 | loss: 0.45541 | train_auc: 0.87495 | valid_auc: 0.85362 |  0:00:10s\n",
      "epoch 17 | loss: 0.46458 | train_auc: 0.87754 | valid_auc: 0.85323 |  0:00:10s\n",
      "epoch 18 | loss: 0.4467  | train_auc: 0.87968 | valid_auc: 0.85758 |  0:00:11s\n",
      "epoch 19 | loss: 0.47862 | train_auc: 0.8824  | valid_auc: 0.85863 |  0:00:11s\n",
      "epoch 20 | loss: 0.4423  | train_auc: 0.88495 | valid_auc: 0.86192 |  0:00:12s\n",
      "epoch 21 | loss: 0.42665 | train_auc: 0.88817 | valid_auc: 0.86074 |  0:00:12s\n",
      "epoch 22 | loss: 0.43197 | train_auc: 0.88833 | valid_auc: 0.86364 |  0:00:13s\n",
      "epoch 23 | loss: 0.43214 | train_auc: 0.8907  | valid_auc: 0.86179 |  0:00:13s\n",
      "epoch 24 | loss: 0.45191 | train_auc: 0.89257 | valid_auc: 0.86232 |  0:00:14s\n",
      "epoch 25 | loss: 0.42902 | train_auc: 0.8926  | valid_auc: 0.85982 |  0:00:14s\n",
      "epoch 26 | loss: 0.42224 | train_auc: 0.89384 | valid_auc: 0.86364 |  0:00:15s\n",
      "epoch 27 | loss: 0.4474  | train_auc: 0.89754 | valid_auc: 0.86403 |  0:00:15s\n",
      "epoch 28 | loss: 0.42266 | train_auc: 0.89857 | valid_auc: 0.86627 |  0:00:16s\n",
      "epoch 29 | loss: 0.41979 | train_auc: 0.89785 | valid_auc: 0.86772 |  0:00:16s\n",
      "epoch 30 | loss: 0.40722 | train_auc: 0.89785 | valid_auc: 0.87194 |  0:00:16s\n",
      "epoch 31 | loss: 0.43085 | train_auc: 0.89918 | valid_auc: 0.86719 |  0:00:17s\n",
      "epoch 32 | loss: 0.40683 | train_auc: 0.8999  | valid_auc: 0.86851 |  0:00:17s\n",
      "epoch 33 | loss: 0.40746 | train_auc: 0.90063 | valid_auc: 0.86733 |  0:00:18s\n",
      "epoch 34 | loss: 0.42846 | train_auc: 0.9015  | valid_auc: 0.87036 |  0:00:18s\n",
      "epoch 35 | loss: 0.42957 | train_auc: 0.90332 | valid_auc: 0.86917 |  0:00:19s\n",
      "epoch 36 | loss: 0.40661 | train_auc: 0.90384 | valid_auc: 0.86812 |  0:00:19s\n",
      "epoch 37 | loss: 0.42299 | train_auc: 0.90456 | valid_auc: 0.87009 |  0:00:20s\n",
      "epoch 38 | loss: 0.41599 | train_auc: 0.9053  | valid_auc: 0.8722  |  0:00:20s\n",
      "epoch 39 | loss: 0.41406 | train_auc: 0.90709 | valid_auc: 0.87286 |  0:00:20s\n",
      "epoch 40 | loss: 0.42129 | train_auc: 0.90608 | valid_auc: 0.87655 |  0:00:21s\n",
      "epoch 41 | loss: 0.41506 | train_auc: 0.90478 | valid_auc: 0.87681 |  0:00:21s\n",
      "epoch 42 | loss: 0.38308 | train_auc: 0.90585 | valid_auc: 0.87918 |  0:00:22s\n",
      "epoch 43 | loss: 0.40521 | train_auc: 0.9055  | valid_auc: 0.87984 |  0:00:22s\n",
      "epoch 44 | loss: 0.40573 | train_auc: 0.90752 | valid_auc: 0.87931 |  0:00:23s\n",
      "epoch 45 | loss: 0.38332 | train_auc: 0.91001 | valid_auc: 0.88261 |  0:00:23s\n",
      "epoch 46 | loss: 0.3835  | train_auc: 0.91068 | valid_auc: 0.87787 |  0:00:23s\n",
      "epoch 47 | loss: 0.39221 | train_auc: 0.91074 | valid_auc: 0.87523 |  0:00:24s\n",
      "epoch 48 | loss: 0.41371 | train_auc: 0.90957 | valid_auc: 0.87049 |  0:00:24s\n",
      "epoch 49 | loss: 0.39172 | train_auc: 0.91064 | valid_auc: 0.86746 |  0:00:25s\n",
      "epoch 50 | loss: 0.39565 | train_auc: 0.91079 | valid_auc: 0.86812 |  0:00:25s\n",
      "epoch 51 | loss: 0.39066 | train_auc: 0.91188 | valid_auc: 0.8693  |  0:00:26s\n",
      "epoch 52 | loss: 0.39465 | train_auc: 0.91118 | valid_auc: 0.87036 |  0:00:26s\n",
      "epoch 53 | loss: 0.41377 | train_auc: 0.91169 | valid_auc: 0.87339 |  0:00:27s\n",
      "epoch 54 | loss: 0.38507 | train_auc: 0.91258 | valid_auc: 0.87655 |  0:00:27s\n",
      "epoch 55 | loss: 0.41311 | train_auc: 0.91224 | valid_auc: 0.87734 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 45 and best_valid_auc = 0.88261\n",
      "0.9676966292134831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8890449438202247\n",
      "0.9297752808988764\n",
      "0.8268156424581006\n",
      "0.8156424581005587\n",
      "0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def convert_data(ofus_matrix):\n",
    "\n",
    "    matrix = np.zeros(ofus_matrix.shape) -1\n",
    "\n",
    "    for i in range(ofus_matrix.shape[1]):\n",
    "        attributes = ofus_matrix[:,i]\n",
    "        if ( isinstance(attributes[0], (int, float)) == False):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(attributes)\n",
    "            matrix[:,i] = le.transform(attributes)\n",
    "        else:\n",
    "            matrix[:,i] = np.nan_to_num(attributes)\n",
    "    return matrix\n",
    "\n",
    "def normalize(ofus_matrix):\n",
    "    matrix = np.zeros(ofus_matrix.shape) -1\n",
    "    \n",
    "    for i in range(ofus_matrix.shape[1]):\n",
    "        attributes = ofus_matrix[:,i]\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        matrix[:,i]=scaler.fit_transform(attributes.reshape([-1,1])).reshape(attributes.shape)\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "def convert(train,test):\n",
    "    r, c = train.shape\n",
    "    rt, ct = test.shape\n",
    "    result = pd.concat([train,test])\n",
    "    new_result= convert_data(result.to_numpy())\n",
    "    new_result=normalize(new_result)\n",
    "    #print(new_result.shape)\n",
    "    new_train =new_result[0:r,]\n",
    "    new_test = new_result[r:r+rt,]\n",
    "    \n",
    "    return (new_train,new_test)\n",
    "\n",
    "\n",
    "def pre2 (train,test) :\n",
    "    numbers_train = train.select_dtypes(np.number)\n",
    "    cad_train = train.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    #age fill nans\n",
    "    train['age_miss'] = train['Age'].isna().astype(int)\n",
    "    test['age_miss'] = test['Age'].isna().astype(int)\n",
    "    \n",
    "    train.loc[train['Survived']==0,'Age'] = train.loc[train['Survived']==0,'Age'].fillna(train.loc[train['Survived']==0,'Age'].median())\n",
    "    train.loc[train['Survived']==1,'Age'] = train.loc[train['Survived']==1,'Age'].fillna(train.loc[train['Survived']==1,'Age'].median())\n",
    "    \n",
    "    test.loc[:,'Age'] = test.loc[:,'Age'].fillna(test.loc[:,'Age'].median())\n",
    "    \n",
    "    #cabbin fill nans\n",
    "    \n",
    "    train['cabin_miss'] = train['Cabin'].isna().astype(int)\n",
    "    test['cabin_miss'] = test['Cabin'].isna().astype(int)\n",
    "    \n",
    "    #train.loc[train['Survived']==0,'Cabin'] = train.loc[train['Survived']==0,'Cabin'].fillna(train.loc[train['Survived']==0,'Cabin'].mode()[0])\n",
    "    #train.loc[train['Survived']==1,'Cabin'] = train.loc[train['Survived']==1,'Cabin'].fillna(train.loc[train['Survived']==1,'Cabin'].mode()[0])\n",
    "    \n",
    "    train.loc[:,'Cabin'] = train.loc[:,'Cabin'].fillna('NInfo')\n",
    "    test.loc[:,'Cabin'] = test.loc[:,'Cabin'].fillna('NInfo')\n",
    "    \n",
    "    #train['Cabin'] = train['Cabin'].str[0]\n",
    "    #test['Cabin'] = test['Cabin'].str[0]\n",
    "    \n",
    "    \n",
    "    #fare fill nans\n",
    "    \n",
    "    train.loc[train['Survived']==0,'Fare'] = train.loc[train['Survived']==0,'Fare'].fillna(train.loc[train['Survived']==0,'Fare'].median())\n",
    "    train.loc[train['Survived']==1,'Fare'] = train.loc[train['Survived']==1,'Fare'].fillna(train.loc[train['Survived']==1,'Fare'].median())\n",
    "    \n",
    "    test.loc[:,'Fare'] = test.loc[:,'Fare'].fillna(test.loc[:,'Fare'].median())\n",
    "    \n",
    "    #embarked fill nans\n",
    "    \n",
    "    train.loc[train['Survived']==0,'Embarked'] = train.loc[train['Survived']==0,'Embarked'].fillna(train.loc[train['Survived']==0,'Embarked'].mode()[0])\n",
    "    train.loc[train['Survived']==1,'Embarked'] = train.loc[train['Survived']==1,'Embarked'].fillna(train.loc[train['Survived']==1,'Embarked'].mode()[0])\n",
    "    \n",
    "    test.loc[:,'Embarked'] = test.loc[:,'Embarked'].fillna(test.loc[:,'Embarked'].mode()[0])\n",
    "    \n",
    "    \n",
    "    #age preprocess\n",
    "    #train['AgeBand'] = train['Age']\n",
    "    \n",
    "    train.loc[train['Age']<5,'Age'] = 1\n",
    "    train.loc[ (train['Age']>=5) & (train['Age']<=14) ,'Age'] = 2\n",
    "    train.loc[ (train['Age']>14) & (train['Age']<=18) ,'Age'] = 3\n",
    "    train.loc[ (train['Age']>18) & (train['Age']<=30) ,'Age'] = 4\n",
    "    train.loc[ (train['Age']>30) & (train['Age']<=60) ,'Age'] = 5\n",
    "    train.loc[train['Age']>60,'Age'] = 6\n",
    "    train['Age'] = train['Age'].astype(int)\n",
    "    \n",
    "    #test['AgeBand'] = test['Age']\n",
    "    \n",
    "    test.loc[test['Age']<5,'Age'] = 1\n",
    "    test.loc[ (test['Age']>=5) & (test['Age']<=14) ,'Age'] = 2\n",
    "    test.loc[ (test['Age']>14) & (test['Age']<=18) ,'Age'] = 3\n",
    "    test.loc[ (test['Age']>18) & (test['Age']<=30) ,'Age'] = 4\n",
    "    test.loc[ (test['Age']>30) & (test['Age']<=60) ,'Age'] = 5\n",
    "    test.loc[test['Age']>60,'Age'] = 6\n",
    "    \n",
    "    test['Age'] = test['Age'].astype(int)\n",
    "    \n",
    "    #FARE preprocess\n",
    "    \n",
    "    #train['FareBand'] = train['Fare']\n",
    "    train.loc[ train['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    train.loc[(train['Fare'] > 8.662) & (train['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    train.loc[(train['Fare'] > 26.0) & (train['Fare'] <= 52), 'Fare']   = 3\n",
    "    train.loc[ train['Fare'] > 52, 'Fare'] = 4\n",
    "    train['Fare'] = train['Fare'].astype(int)\n",
    "    \n",
    "    #test['FareBand'] = test['Fare']\n",
    "    test.loc[ test['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    test.loc[(test['Fare'] > 8.662) & (test['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    test.loc[(test['Fare'] > 26.0) & (test['Fare'] <= 52), 'Fare']   = 3\n",
    "    test.loc[ test['Fare'] > 52, 'Fare'] = 4\n",
    "    test['Fare'] = test['Fare'].astype(int)\n",
    "    \n",
    "    #NAME preprocess\n",
    "    \n",
    "    train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    test['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "        \n",
    "    train['Name'] = train['Name'].replace(['Lady', 'Countess','Capt', 'Col',\n",
    "                                           'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    train['Name'] = train['Name'].replace('Mlle', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Ms', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "    test['Name'] = test['Name'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test['Name'] = test['Name'].replace('Mlle', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Ms', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    #Parch and SibSp\n",
    "    train['Alone'] = train['Parch'] + train['SibSp']\n",
    "    train.loc[ train['Alone'] > 0 ,'Alone'] =1\n",
    "    \n",
    "    test['Alone'] = test['Parch'] + test['SibSp']\n",
    "    test.loc[ test['Alone']>0,'Alone'] =1\n",
    "    \n",
    "    #DELETE\n",
    "    train = train.drop(['Survived','PassengerId'], axis=1)\n",
    "    test = test.drop(['PassengerId'], axis=1)\n",
    "    \n",
    "    return (train,test)\n",
    "\n",
    "def pre (train,test) :\n",
    "    NUMERIC_COLUMNS = train.select_dtypes(np.number)\n",
    "    CATEGORICAL_COLUMNS = train.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    #FILL NANS TRAIN DATA\n",
    "    \n",
    "    for num in NUMERIC_COLUMNS:\n",
    "        if (num != 'Survived') :\n",
    "            train.loc[train['Survived']==0,num] = train.loc[train['Survived']==0,num].fillna(train.loc[train['Survived']==0,num].median())\n",
    "            train.loc[train['Survived']==1,num] = train.loc[train['Survived']==1,num].fillna(train.loc[train['Survived']==1,num].median())\n",
    "    \n",
    "    for cat in CATEGORICAL_COLUMNS:\n",
    "        train.loc[train['Survived']==0,cat] = train.loc[train['Survived']==0,cat].fillna(train.loc[train['Survived']==0,cat].mode()[0])\n",
    "        train.loc[train['Survived']==1,cat] = train.loc[train['Survived']==1,cat].fillna(train.loc[train['Survived']==1,cat].mode()[0])\n",
    "    \n",
    "    #ERASE SURVIVED COLUM\n",
    "    \n",
    "    train  = train.drop(['Survived'], axis=1)\n",
    "    \n",
    "    #FILL NANS TEST DATA\n",
    "    data = pd.concat([train,test])\n",
    "    \n",
    "    NUMERIC_COLUMNS = data.select_dtypes(np.number)\n",
    "    CATEGORICAL_COLUMNS = data.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    for num in NUMERIC_COLUMNS:\n",
    "        test.loc[:,num] = test.loc[:,num].fillna(data.loc[:,num].median())\n",
    "    \n",
    "    for cat in CATEGORICAL_COLUMNS:\n",
    "        test.loc[:,cat] = test.loc[:,cat].fillna(data.loc[:,cat].mode()[0])\n",
    "    \n",
    "    \n",
    "    #AGE\n",
    "    \"\"\"\n",
    "    train.loc[ (train['Age']>=0) & (train['Age']<=25) ,'Age'] = 1\n",
    "    train.loc[ (train['Age']>25) & (train['Age']<=31) ,'Age'] = 2\n",
    "    train.loc[train['Age']>31,'Age'] = 3\n",
    "    train['Age'] = train['Age'].astype(int)\n",
    "    \n",
    "\n",
    "    test.loc[ (test['Age']>=0) & (test['Age']<=25) ,'Age'] = 1\n",
    "    test.loc[ (test['Age']>25) & (test['Age']<=31) ,'Age'] = 2\n",
    "    test.loc[test['Age']>31,'Age'] = 3\n",
    "    test['Age'] = test['Age'].astype(int)\n",
    "    \n",
    "    #FARE\n",
    "    \n",
    "    train.loc[ train['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    train.loc[(train['Fare'] > 8.662) & (train['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    train.loc[(train['Fare'] > 26.0), 'Fare']   = 3\n",
    "\n",
    "    train['Fare'] = train['Fare'].astype(int)\n",
    "    \n",
    "    test.loc[ test['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    test.loc[(test['Fare'] > 8.662) & (test['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    test.loc[(test['Fare'] > 26.0) , 'Fare']   = 3\n",
    "\n",
    "    test['Fare'] = train['Fare'].astype(int)\n",
    "    \"\"\"\n",
    "    \n",
    "    #NAME\n",
    "    \n",
    "    train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    test['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "        \n",
    "    train['Name'] = train['Name'].replace(['Lady', 'Countess','Capt', 'Col',\n",
    "                                           'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    train['Name'] = train['Name'].replace('Mlle', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Ms', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "    test['Name'] = test['Name'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test['Name'] = test['Name'].replace('Mlle', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Ms', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    #Parch and SibSp\n",
    "    train['Alone'] = train['Parch'] + train['SibSp']\n",
    "    train.loc[ train['Alone'] > 0 ,'Alone'] =1\n",
    "    \n",
    "    test['Alone'] = test['Parch'] + test['SibSp']\n",
    "    test.loc[ test['Alone']>0,'Alone'] =1\n",
    "    \n",
    "    #DELETE\n",
    "    train = train.drop(['Cabin','Ticket','PassengerId'], axis=1)\n",
    "    test = test.drop(['Cabin','Ticket','PassengerId'], axis=1)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def fine_tune_xgb_sklearn(X_train, Y_train, seed):\n",
    "    RS_CV = 5\n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 30)]\n",
    "                    \n",
    "    \n",
    "    random_grid = {\n",
    "                 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
    "                 'max_depth' : [ 3, 4, 5, 6, 8, 10,15],\n",
    "                 'min_child_weight' : [ 1, 3, 5, 7,9],\n",
    "                 'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ,1],\n",
    "                 'colsample_bytree': [ 0.3, 0.4, 0.5 ,0.6,0.7,1],\n",
    "                 'n_estimators': n_estimators\n",
    "                }\n",
    "    \n",
    "     \n",
    "    clf_xgb = XGBClassifier(#n_estimators =3000,\n",
    "                            verbosity=0,\n",
    "                            objective='binary:logistic',\n",
    "                            booster='gbtree',\n",
    "                            #n_jobs=-1,\n",
    "                            #nthread=None,\n",
    "                            #max_delta_step=0,\n",
    "                            subsample=0.7,\n",
    "                            #colsample_bylevel=1,\n",
    "                            #colsample_bynode=1,\n",
    "                            #reg_alpha=0,\n",
    "                            #reg_lambda=1,\n",
    "                            #scale_pos_weight=1,\n",
    "                            #base_score=0.5,\n",
    "                            #random_state=0,\n",
    "                            verbose=0,\n",
    "                            #seed=None\n",
    "                           )\n",
    "\n",
    "    \n",
    "    clf_random = RandomizedSearchCV(estimator = clf_xgb,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0,\n",
    "                                    random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "\n",
    "def fine_tune_RCF_sklearn(X_train, Y_train, seed):\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 14)]\n",
    "    max_features = ['sqrt','log2']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 55, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state = seed)\n",
    "    clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 2, cv = 5, verbose=2, random_state=seed, n_jobs = -1)\n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "    \n",
    "def fine_tune_cat_sklearn(X_train, Y_train, seed):\n",
    "    \n",
    "    RS_CV = 5 \n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "\n",
    "    iterations = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 5)]\n",
    "    \n",
    "    random_grid = {\n",
    "                   'depth':[8,10],\n",
    "                   'learning_rate': [0.1,0.01],\n",
    "                   'iterations': iterations\n",
    "                   }\n",
    "    \n",
    "    clf = CatBoostClassifier(\n",
    "                            #iterations=1000,         # Reduced iterations\n",
    "                            l2_leaf_reg=3.0,         # Increased L2 regularization term\n",
    "                            #eval_metric='Accuracy',\n",
    "                            random_seed=seed,\n",
    "                            verbose=0,\n",
    "                            #loss_function ='accuracy'\n",
    "                           )\n",
    "\n",
    "    clf_random = RandomizedSearchCV(estimator = clf,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0, random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "\n",
    "def tab_net(trainx,trainy,seed):\n",
    "    X_train, X_valid, Y_train,Y_valid = train_test_split(trainx, trainy, test_size = 0.2, random_state =seed,stratify=trainy)\n",
    "    \n",
    "    tb_cls = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                               optimizer_params=dict(lr=1e-3),\n",
    "                               scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "                               scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                               verbose=1,\n",
    "                               seed=seed,\n",
    "                               mask_type='entmax' # \"sparsemax\" entmax\n",
    "                               )\n",
    "\n",
    "    tb_cls.fit(X_train,Y_train,\n",
    "                               eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "                               eval_name=['train', 'valid'],\n",
    "                               eval_metric=['auc'],\n",
    "                               max_epochs=100 , patience=10,\n",
    "                               batch_size=32, drop_last=False)    \n",
    "    return tb_cls\n",
    "\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test = pd.read_csv('titanic/test.csv')\n",
    "submission = pd.read_csv('titanic/sub.csv')\n",
    "\n",
    "Y_train = train['Survived'].to_numpy()\n",
    "\n",
    "(train,test) = pre2(train,test)\n",
    "\n",
    "(X_train,X_test)=convert(train,test)\n",
    "\n",
    "x=X_train.copy()\n",
    "y=Y_train.copy()\n",
    "seed = 13\n",
    "X_train, X_valid, Y_train,Y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state =seed,stratify=Y_train)\n",
    "\n",
    "model1 = fine_tune_xgb_sklearn ( X_train,Y_train, seed)\n",
    "model2 = fine_tune_RCF_sklearn ( X_train,Y_train, seed)\n",
    "model3 = fine_tune_cat_sklearn ( X_train,Y_train, seed)\n",
    "model4 = tab_net(x,y,seed)\n",
    "    \n",
    "# Test model and generate prediction\n",
    "\n",
    "print(model1.score(X_train,Y_train))\n",
    "print(model2.score(X_train,Y_train))\n",
    "print(model3.score(X_train,Y_train))\n",
    "\n",
    "print(model1.score(X_valid,Y_valid))\n",
    "print(model2.score(X_valid,Y_valid))\n",
    "print(model3.score(X_valid,Y_valid))\n",
    "\n",
    "result1= model1.predict(X_test)\n",
    "result2= model2.predict(X_test)\n",
    "result3= model3.predict(X_test)\n",
    "result4= model4.predict(X_test)\n",
    "\n",
    "submission['Survived'] =result1\n",
    "name = 'submission01.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result2\n",
    "name = 'submission02.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result3\n",
    "name = 'submission03.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result4\n",
    "name = 'submission04.csv'\n",
    "submission.to_csv(name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.redcrab-software.com/en/Calculator/Softmax\n",
    "\n",
    "w1 = 0.35\n",
    "w2 = 0.33\n",
    "w3 = 0.32\n",
    "\n",
    "df1 = pd.read_csv('submission01.csv')\n",
    "df2 = pd.read_csv('submission02.csv')\n",
    "df3 = pd.read_csv('submission03.csv')\n",
    "\n",
    "res = df1.values[:,1]*w1 + df2.values[:,1]*w2 + df3.values[:,1]*w3 \n",
    "\n",
    "df_final = df1.copy()\n",
    "df_final['Survived'] = res\n",
    "df_final.loc[ df_final['Survived'] <0.5, 'Survived'] = 0\n",
    "df_final.loc[ df_final['Survived'] >=0.5, 'Survived'] = 1\n",
    "df_final['Survived']=df_final['Survived'].astype('int32')\n",
    "df_final.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
