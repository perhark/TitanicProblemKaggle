{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 184, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.25, 'gamma': 0.0, 'colsample_bytree': 0.7}\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    1.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 446, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 15, 'bootstrap': True}\n",
      "{'learning_rate': 0.01, 'iterations': 500, 'depth': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.90063 | train_auc: 0.5909  | valid_auc: 0.61925 |  0:00:00s\n",
      "epoch 1  | loss: 0.72794 | train_auc: 0.6313  | valid_auc: 0.68142 |  0:00:01s\n",
      "epoch 2  | loss: 0.64531 | train_auc: 0.65335 | valid_auc: 0.71058 |  0:00:01s\n",
      "epoch 3  | loss: 0.60637 | train_auc: 0.63301 | valid_auc: 0.70752 |  0:00:02s\n",
      "epoch 4  | loss: 0.61277 | train_auc: 0.62269 | valid_auc: 0.69326 |  0:00:02s\n",
      "epoch 5  | loss: 0.57948 | train_auc: 0.66224 | valid_auc: 0.72636 |  0:00:03s\n",
      "epoch 6  | loss: 0.52373 | train_auc: 0.72049 | valid_auc: 0.75055 |  0:00:03s\n",
      "epoch 7  | loss: 0.50853 | train_auc: 0.7816  | valid_auc: 0.78493 |  0:00:04s\n",
      "epoch 8  | loss: 0.48382 | train_auc: 0.81493 | valid_auc: 0.82244 |  0:00:05s\n",
      "epoch 9  | loss: 0.49614 | train_auc: 0.82919 | valid_auc: 0.83869 |  0:00:05s\n",
      "epoch 10 | loss: 0.47627 | train_auc: 0.85541 | valid_auc: 0.85283 |  0:00:06s\n",
      "epoch 11 | loss: 0.47584 | train_auc: 0.87064 | valid_auc: 0.83933 |  0:00:06s\n",
      "epoch 12 | loss: 0.45241 | train_auc: 0.88618 | valid_auc: 0.84366 |  0:00:07s\n",
      "epoch 13 | loss: 0.41834 | train_auc: 0.89343 | valid_auc: 0.84926 |  0:00:07s\n",
      "epoch 14 | loss: 0.43278 | train_auc: 0.90344 | valid_auc: 0.86072 |  0:00:08s\n",
      "epoch 15 | loss: 0.40088 | train_auc: 0.91243 | valid_auc: 0.86751 |  0:00:08s\n",
      "epoch 16 | loss: 0.46195 | train_auc: 0.9165  | valid_auc: 0.86878 |  0:00:09s\n",
      "epoch 17 | loss: 0.39678 | train_auc: 0.92243 | valid_auc: 0.87116 |  0:00:09s\n",
      "epoch 18 | loss: 0.39158 | train_auc: 0.9259  | valid_auc: 0.88202 |  0:00:10s\n",
      "epoch 19 | loss: 0.38673 | train_auc: 0.93223 | valid_auc: 0.8883  |  0:00:11s\n",
      "epoch 20 | loss: 0.34682 | train_auc: 0.93765 | valid_auc: 0.89467 |  0:00:11s\n",
      "epoch 21 | loss: 0.38507 | train_auc: 0.93911 | valid_auc: 0.89543 |  0:00:12s\n",
      "epoch 22 | loss: 0.38781 | train_auc: 0.9436  | valid_auc: 0.90282 |  0:00:12s\n",
      "epoch 23 | loss: 0.37216 | train_auc: 0.9451  | valid_auc: 0.89857 |  0:00:13s\n",
      "epoch 24 | loss: 0.34538 | train_auc: 0.9499  | valid_auc: 0.9046  |  0:00:13s\n",
      "epoch 25 | loss: 0.38933 | train_auc: 0.95044 | valid_auc: 0.90205 |  0:00:14s\n",
      "epoch 26 | loss: 0.31805 | train_auc: 0.95611 | valid_auc: 0.91275 |  0:00:14s\n",
      "epoch 27 | loss: 0.32122 | train_auc: 0.95983 | valid_auc: 0.91309 |  0:00:15s\n",
      "epoch 28 | loss: 0.34185 | train_auc: 0.96064 | valid_auc: 0.91682 |  0:00:16s\n",
      "epoch 29 | loss: 0.35081 | train_auc: 0.96037 | valid_auc: 0.91181 |  0:00:16s\n",
      "epoch 30 | loss: 0.31957 | train_auc: 0.96221 | valid_auc: 0.9136  |  0:00:17s\n",
      "epoch 31 | loss: 0.30598 | train_auc: 0.96383 | valid_auc: 0.91665 |  0:00:17s\n",
      "epoch 32 | loss: 0.31127 | train_auc: 0.96652 | valid_auc: 0.91801 |  0:00:18s\n",
      "epoch 33 | loss: 0.31169 | train_auc: 0.96743 | valid_auc: 0.92141 |  0:00:18s\n",
      "epoch 34 | loss: 0.29762 | train_auc: 0.96996 | valid_auc: 0.92438 |  0:00:19s\n",
      "epoch 35 | loss: 0.29581 | train_auc: 0.97196 | valid_auc: 0.9276  |  0:00:19s\n",
      "epoch 36 | loss: 0.28177 | train_auc: 0.97018 | valid_auc: 0.92828 |  0:00:20s\n",
      "epoch 37 | loss: 0.27918 | train_auc: 0.97165 | valid_auc: 0.93066 |  0:00:21s\n",
      "epoch 38 | loss: 0.29166 | train_auc: 0.97341 | valid_auc: 0.93269 |  0:00:21s\n",
      "epoch 39 | loss: 0.28198 | train_auc: 0.9737  | valid_auc: 0.93269 |  0:00:22s\n",
      "epoch 40 | loss: 0.30199 | train_auc: 0.97293 | valid_auc: 0.93091 |  0:00:22s\n",
      "epoch 41 | loss: 0.29004 | train_auc: 0.97466 | valid_auc: 0.9304  |  0:00:23s\n",
      "epoch 42 | loss: 0.29467 | train_auc: 0.97525 | valid_auc: 0.931   |  0:00:23s\n",
      "epoch 43 | loss: 0.23526 | train_auc: 0.97464 | valid_auc: 0.93346 |  0:00:24s\n",
      "epoch 44 | loss: 0.26189 | train_auc: 0.97256 | valid_auc: 0.92684 |  0:00:24s\n",
      "epoch 45 | loss: 0.27138 | train_auc: 0.97564 | valid_auc: 0.93388 |  0:00:25s\n",
      "epoch 46 | loss: 0.25504 | train_auc: 0.97676 | valid_auc: 0.93388 |  0:00:25s\n",
      "epoch 47 | loss: 0.25625 | train_auc: 0.97959 | valid_auc: 0.93626 |  0:00:26s\n",
      "epoch 48 | loss: 0.24575 | train_auc: 0.97986 | valid_auc: 0.93923 |  0:00:27s\n",
      "epoch 49 | loss: 0.22745 | train_auc: 0.97994 | valid_auc: 0.9405  |  0:00:27s\n",
      "epoch 50 | loss: 0.25485 | train_auc: 0.98003 | valid_auc: 0.94169 |  0:00:28s\n",
      "epoch 51 | loss: 0.23405 | train_auc: 0.98131 | valid_auc: 0.94424 |  0:00:28s\n",
      "epoch 52 | loss: 0.22785 | train_auc: 0.98139 | valid_auc: 0.94755 |  0:00:29s\n",
      "epoch 53 | loss: 0.23956 | train_auc: 0.98268 | valid_auc: 0.94967 |  0:00:29s\n",
      "epoch 54 | loss: 0.24475 | train_auc: 0.9834  | valid_auc: 0.95026 |  0:00:30s\n",
      "epoch 55 | loss: 0.2444  | train_auc: 0.98363 | valid_auc: 0.95468 |  0:00:30s\n",
      "epoch 56 | loss: 0.26577 | train_auc: 0.9837  | valid_auc: 0.95434 |  0:00:31s\n",
      "epoch 57 | loss: 0.26735 | train_auc: 0.98418 | valid_auc: 0.95612 |  0:00:32s\n",
      "epoch 58 | loss: 0.21679 | train_auc: 0.98491 | valid_auc: 0.95179 |  0:00:32s\n",
      "epoch 59 | loss: 0.24296 | train_auc: 0.98612 | valid_auc: 0.95196 |  0:00:33s\n",
      "epoch 60 | loss: 0.22669 | train_auc: 0.98656 | valid_auc: 0.95586 |  0:00:33s\n",
      "epoch 61 | loss: 0.23624 | train_auc: 0.98668 | valid_auc: 0.95629 |  0:00:34s\n",
      "epoch 62 | loss: 0.21879 | train_auc: 0.98763 | valid_auc: 0.95731 |  0:00:34s\n",
      "epoch 63 | loss: 0.21565 | train_auc: 0.98893 | valid_auc: 0.9568  |  0:00:35s\n",
      "epoch 64 | loss: 0.23508 | train_auc: 0.98821 | valid_auc: 0.95578 |  0:00:35s\n",
      "epoch 65 | loss: 0.2727  | train_auc: 0.99002 | valid_auc: 0.95383 |  0:00:36s\n",
      "epoch 66 | loss: 0.23922 | train_auc: 0.98958 | valid_auc: 0.9557  |  0:00:36s\n",
      "epoch 67 | loss: 0.23581 | train_auc: 0.98777 | valid_auc: 0.95476 |  0:00:37s\n",
      "epoch 68 | loss: 0.23248 | train_auc: 0.9878  | valid_auc: 0.95442 |  0:00:38s\n",
      "epoch 69 | loss: 0.2265  | train_auc: 0.98683 | valid_auc: 0.95349 |  0:00:38s\n",
      "epoch 70 | loss: 0.20142 | train_auc: 0.98913 | valid_auc: 0.9557  |  0:00:39s\n",
      "epoch 71 | loss: 0.18153 | train_auc: 0.98936 | valid_auc: 0.95578 |  0:00:39s\n",
      "epoch 72 | loss: 0.23151 | train_auc: 0.98954 | valid_auc: 0.95748 |  0:00:40s\n",
      "epoch 73 | loss: 0.23475 | train_auc: 0.98987 | valid_auc: 0.95637 |  0:00:40s\n",
      "epoch 74 | loss: 0.22965 | train_auc: 0.98954 | valid_auc: 0.95205 |  0:00:41s\n",
      "epoch 75 | loss: 0.21195 | train_auc: 0.98885 | valid_auc: 0.9523  |  0:00:41s\n",
      "epoch 76 | loss: 0.21396 | train_auc: 0.98887 | valid_auc: 0.95472 |  0:00:42s\n",
      "epoch 77 | loss: 0.20016 | train_auc: 0.98914 | valid_auc: 0.95468 |  0:00:43s\n",
      "epoch 78 | loss: 0.20833 | train_auc: 0.99048 | valid_auc: 0.95595 |  0:00:43s\n",
      "epoch 79 | loss: 0.21235 | train_auc: 0.98973 | valid_auc: 0.95391 |  0:00:44s\n",
      "epoch 80 | loss: 0.21932 | train_auc: 0.98996 | valid_auc: 0.95238 |  0:00:44s\n",
      "epoch 81 | loss: 0.19238 | train_auc: 0.99037 | valid_auc: 0.95315 |  0:00:45s\n",
      "epoch 82 | loss: 0.23829 | train_auc: 0.99011 | valid_auc: 0.95196 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 72 and best_valid_auc = 0.95748\n",
      "0.9975031210986267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625468164794008\n",
      "0.9962546816479401\n",
      "0.9666666666666667\n",
      "0.9111111111111111\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "\n",
    "def convert_data(ofus_matrix):\n",
    "\n",
    "    matrix = np.zeros(ofus_matrix.shape) -1\n",
    "\n",
    "    for i in range(ofus_matrix.shape[1]):\n",
    "        attributes = ofus_matrix[:,i]\n",
    "        if ( isinstance(attributes[0], (int, float)) == False):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(attributes)\n",
    "            matrix[:,i] = le.transform(attributes)\n",
    "        else:\n",
    "            matrix[:,i] = np.nan_to_num(attributes)\n",
    "    return matrix\n",
    "\n",
    "def convert(train,test):\n",
    "    r, c = train.shape\n",
    "    rt, ct = test.shape\n",
    "    result = pd.concat([train,test])\n",
    "    new_result= convert_data(result.to_numpy())\n",
    "    #print(new_result.shape)\n",
    "    new_train =new_result[0:r,]\n",
    "    new_test = new_result[r:r+rt,]\n",
    "    \n",
    "    return (new_train,new_test)\n",
    "    \n",
    "\n",
    "def pre (train,test) :\n",
    "    NUMERIC_COLUMNS = train.select_dtypes(np.number)\n",
    "    CATEGORICAL_COLUMNS = train.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    #FILL NANS TRAIN DATA\n",
    "    \n",
    "    for num in NUMERIC_COLUMNS:\n",
    "        if (num != 'Survived') :\n",
    "            train.loc[train['Survived']==0,num] = train.loc[train['Survived']==0,num].fillna(train.loc[train['Survived']==0,num].median())\n",
    "            train.loc[train['Survived']==1,num] = train.loc[train['Survived']==1,num].fillna(train.loc[train['Survived']==1,num].median())\n",
    "    \n",
    "    for cat in CATEGORICAL_COLUMNS:\n",
    "        train.loc[train['Survived']==0,cat] = train.loc[train['Survived']==0,cat].fillna(train.loc[train['Survived']==0,cat].mode()[0])\n",
    "        train.loc[train['Survived']==1,cat] = train.loc[train['Survived']==1,cat].fillna(train.loc[train['Survived']==1,cat].mode()[0])\n",
    "    \n",
    "    #ERASE SURVIVED COLUM\n",
    "    \n",
    "    train  = train.drop(['Survived'], axis=1)\n",
    "    \n",
    "    #FILL NANS TEST DATA\n",
    "    data = pd.concat([train,test])\n",
    "    \n",
    "    NUMERIC_COLUMNS = data.select_dtypes(np.number)\n",
    "    CATEGORICAL_COLUMNS = data.select_dtypes(exclude=[np.number])\n",
    "    \n",
    "    for num in NUMERIC_COLUMNS:\n",
    "        test.loc[:,num] = test.loc[:,num].fillna(data.loc[:,num].median())\n",
    "    \n",
    "    for cat in CATEGORICAL_COLUMNS:\n",
    "        test.loc[:,cat] = test.loc[:,cat].fillna(data.loc[:,cat].mode()[0])\n",
    "    \n",
    "    \n",
    "    #AGE\n",
    "    \n",
    "    train.loc[ (train['Age']>=0) & (train['Age']<=25) ,'Age'] = 1\n",
    "    train.loc[ (train['Age']>25) & (train['Age']<=31) ,'Age'] = 2\n",
    "    train.loc[train['Age']>31,'Age'] = 3\n",
    "    train['Age'] = train['Age'].astype(int)\n",
    "    \n",
    "\n",
    "    test.loc[ (test['Age']>=0) & (test['Age']<=25) ,'Age'] = 1\n",
    "    test.loc[ (test['Age']>25) & (test['Age']<=31) ,'Age'] = 2\n",
    "    test.loc[test['Age']>31,'Age'] = 3\n",
    "    test['Age'] = test['Age'].astype(int)\n",
    "    \n",
    "    #FARE\n",
    "    \n",
    "    train.loc[ train['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    train.loc[(train['Fare'] > 8.662) & (train['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    train.loc[(train['Fare'] > 26.0), 'Fare']   = 3\n",
    "\n",
    "    train['Fare'] = train['Fare'].astype(int)\n",
    "    \n",
    "    test.loc[ test['Fare'] <= 8.662, 'Fare'] = 1\n",
    "    test.loc[(test['Fare'] > 8.662) & (test['Fare'] <= 26.0), 'Fare'] = 2\n",
    "    test.loc[(test['Fare'] > 26.0) , 'Fare']   = 3\n",
    "\n",
    "    test['Fare'] = train['Fare'].astype(int)\n",
    "    \n",
    "    \n",
    "    #NAME\n",
    "    \n",
    "    train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    test['Name'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "        \n",
    "    train['Name'] = train['Name'].replace(['Lady', 'Countess','Capt', 'Col',\n",
    "                                           'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    train['Name'] = train['Name'].replace('Mlle', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Ms', 'Miss')\n",
    "    train['Name'] = train['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "    test['Name'] = test['Name'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test['Name'] = test['Name'].replace('Mlle', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Ms', 'Miss')\n",
    "    test['Name'] = test['Name'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    #Parch and SibSp\n",
    "    train['Alone'] = train['Parch'] + train['SibSp']\n",
    "    train.loc[ train['Alone'] > 0 ,'Alone'] =1\n",
    "    \n",
    "    test['Alone'] = test['Parch'] + test['SibSp']\n",
    "    test.loc[ test['Alone']>0,'Alone'] =1\n",
    "    \n",
    "    #DELETE\n",
    "    train = train.drop(['Ticket','PassengerId'], axis=1)\n",
    "    test = test.drop(['Ticket','PassengerId'], axis=1)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def fine_tune_xgb_sklearn(X_train, Y_train, seed):\n",
    "    RS_CV = 5\n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 30)]\n",
    "                    \n",
    "    \n",
    "    random_grid = {\n",
    "                 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
    "                 'max_depth' : [ 3, 4, 5, 6, 8, 10,15,20],\n",
    "                 'min_child_weight' : [ 1, 3, 5, 7,9],\n",
    "                 'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ,1],\n",
    "                 'colsample_bytree': [ 0.3, 0.4, 0.5 ,0.6,0.7,1],\n",
    "                 'n_estimators': n_estimators\n",
    "                }\n",
    "    \n",
    "     \n",
    "    clf_xgb = XGBClassifier(#n_estimators =3000,\n",
    "                            verbosity=0,\n",
    "                            objective='binary:logistic',\n",
    "                            booster='gbtree',\n",
    "                            #n_jobs=-1,\n",
    "                            #nthread=None,\n",
    "                            #max_delta_step=0,\n",
    "                            subsample=0.7,\n",
    "                            #colsample_bylevel=1,\n",
    "                            #colsample_bynode=1,\n",
    "                            #reg_alpha=0,\n",
    "                            #reg_lambda=1,\n",
    "                            #scale_pos_weight=1,\n",
    "                            #base_score=0.5,\n",
    "                            #random_state=0,\n",
    "                            verbose=0,\n",
    "                            #seed=None\n",
    "                           )\n",
    "\n",
    "    \n",
    "    clf_random = RandomizedSearchCV(estimator = clf_xgb,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0,\n",
    "                                    random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "\n",
    "def fine_tune_RCF_sklearn(X_train, Y_train, seed):\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 14)]\n",
    "    max_features = ['sqrt','log2']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 55, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state = seed)\n",
    "    clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 2, cv = 5, verbose=2, random_state=seed, n_jobs = -1)\n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "    \n",
    "def fine_tune_cat_sklearn(X_train, Y_train, seed):\n",
    "    \n",
    "    RS_CV = 5 \n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "\n",
    "    iterations = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 5)]\n",
    "    \n",
    "    random_grid = {\n",
    "                   'depth':[8,10],\n",
    "                   'learning_rate': [0.1,0.01],\n",
    "                   'iterations': iterations\n",
    "                   }\n",
    "    \n",
    "    clf = CatBoostClassifier(\n",
    "                            #iterations=1000,         # Reduced iterations\n",
    "                            l2_leaf_reg=3.0,         # Increased L2 regularization term\n",
    "                            #eval_metric='Accuracy',\n",
    "                            random_seed=seed,\n",
    "                            verbose=0,\n",
    "                            #loss_function ='accuracy'\n",
    "                           )\n",
    "\n",
    "    clf_random = RandomizedSearchCV(estimator = clf,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0, random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "\n",
    "def tab_net(trainx,trainy,seed):\n",
    "    X_train, X_valid, Y_train,Y_valid = train_test_split(trainx, trainy, test_size = 0.25, random_state =seed,stratify=trainy)\n",
    "    \n",
    "    tb_cls = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                               optimizer_params=dict(lr=1e-3),\n",
    "                               scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "                               scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                               verbose=1,\n",
    "                               seed=seed,\n",
    "                               mask_type='entmax' # \"sparsemax\" entmax\n",
    "                               )\n",
    "\n",
    "    tb_cls.fit(X_train,Y_train,\n",
    "                               eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "                               eval_name=['train', 'valid'],\n",
    "                               eval_metric=['auc'],\n",
    "                               max_epochs=100 , patience=10,\n",
    "                               batch_size=32, drop_last=False)    \n",
    "    return tb_cls\n",
    "\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test = pd.read_csv('titanic/test.csv')\n",
    "submission = pd.read_csv('titanic/sub.csv')\n",
    "\n",
    "Y_train = train['Survived'].to_numpy()\n",
    "\n",
    "(train,test) = pre(train,test)\n",
    "\n",
    "#print(pd.crosstab(train['Name'], train['Sex']))\n",
    "#print(train[['Name', 'Survived']].groupby(['Name'], as_index=False).mean())\n",
    "\n",
    "\n",
    "(X_train,X_test)=convert(train,test)\n",
    "\n",
    "x=X_train.copy()\n",
    "y=Y_train.copy()\n",
    "seed = 13\n",
    "X_train, X_valid, Y_train,Y_valid = train_test_split(X_train, Y_train, test_size = 0.1, random_state =seed,stratify=Y_train)\n",
    "\n",
    "model1 = fine_tune_xgb_sklearn ( X_train,Y_train, seed)\n",
    "model2 = fine_tune_RCF_sklearn ( X_train,Y_train, seed)\n",
    "model3 = fine_tune_cat_sklearn ( X_train,Y_train, seed)\n",
    "model4 = tab_net(x,y,seed)\n",
    "    \n",
    "# Test model and generate prediction\n",
    "\n",
    "print(model1.score(X_train,Y_train))\n",
    "print(model2.score(X_train,Y_train))\n",
    "print(model3.score(X_train,Y_train))\n",
    "\n",
    "print(model1.score(X_valid,Y_valid))\n",
    "print(model2.score(X_valid,Y_valid))\n",
    "print(model3.score(X_valid,Y_valid))\n",
    "\n",
    "result1= model1.predict(X_test)\n",
    "result2= model2.predict(X_test)\n",
    "result3= model3.predict(X_test)\n",
    "result4= model4.predict(X_test)\n",
    "\n",
    "submission['Survived'] =result1\n",
    "name = 'submission01.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result2\n",
    "name = 'submission02.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result3\n",
    "name = 'submission03.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Survived'] =result4\n",
    "name = 'submission04.csv'\n",
    "submission.to_csv(name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.redcrab-software.com/en/Calculator/Softmax\n",
    "\n",
    "w1 = 0.35\n",
    "w2 = 0.33\n",
    "w3 = 0.32\n",
    "\n",
    "df1 = pd.read_csv('submission01.csv')\n",
    "df2 = pd.read_csv('submission02.csv')\n",
    "df3 = pd.read_csv('submission03.csv')\n",
    "\n",
    "res = df1.values[:,1]*w1 + df2.values[:,1]*w2 + df3.values[:,1]*w3 \n",
    "\n",
    "df_final = df1.copy()\n",
    "df_final['Survived'] = res\n",
    "df_final.loc[ df_final['Survived'] <0.5, 'Survived'] = 0\n",
    "df_final.loc[ df_final['Survived'] >=0.5, 'Survived'] = 1\n",
    "df_final['Survived']=df_final['Survived'].astype('int32')\n",
    "df_final.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
